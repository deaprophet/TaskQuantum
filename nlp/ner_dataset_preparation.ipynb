{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain NER Dataset Preparation\n",
    "\n",
    "Convert raw CSV data with character markers to NER format with BIO tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1584\n",
      "\n",
      "Columns: ['text', 'marker']\n",
      "\n",
      "First row:\n",
      "text      A visit to a science museum for hands-on learn...\n",
      "marker                                                   []\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/raw/mountain_dataset_with_markup.csv')\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst row:\")\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The Carpathian Mountains are a vital part of Europe's natural heritage.\n",
      "Markers: [(4, 24)]\n",
      "  Mountain: 'Carpathian Mountains'\n"
     ]
    }
   ],
   "source": [
    "def parse_marker(marker_str):\n",
    "    \"\"\"Parse marker string to extract character positions\"\"\"\n",
    "    if marker_str == '[]' or not marker_str:\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(marker_str)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "text = df.iloc[22]['text']\n",
    "markers = parse_marker(df.iloc[22]['marker'])\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Markers: {markers}\")\n",
    "\n",
    "if markers:\n",
    "    for start, end in markers:\n",
    "        print(f\"  Mountain: '{text[start:end]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to BIO Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level NER tags:\n",
      "  The                  O\n",
      "  Carpathian           B-MOUNTAIN\n",
      "  Mountains            I-MOUNTAIN\n",
      "  are                  O\n",
      "  a                    O\n",
      "  vital                O\n",
      "  part                 O\n",
      "  of                   O\n",
      "  Europe's             O\n",
      "  natural              O\n",
      "  heritage.            O\n"
     ]
    }
   ],
   "source": [
    "def create_ner_tags(text, markers):\n",
    "    \"\"\"Convert text and character markers to NER BIO tags\"\"\"\n",
    "    tokens = text.split()\n",
    "    ner_tags = ['O'] * len(tokens)\n",
    "    \n",
    "    if not markers:\n",
    "        return tokens, ner_tags\n",
    "    \n",
    "    markers = sorted(markers, key=lambda x: x[0])\n",
    "    \n",
    "    # Create list of tokens with their character positions\n",
    "    token_positions = []\n",
    "    char_pos = 0\n",
    "    for token in tokens:\n",
    "        start = char_pos\n",
    "        end = char_pos + len(token)\n",
    "        token_positions.append((start, end))\n",
    "        char_pos = end + 1\n",
    "    \n",
    "    # For each marker, find corresponding tokens\n",
    "    for start, end in markers:\n",
    "        entity_tokens = []\n",
    "        \n",
    "        for i, (token_start, token_end) in enumerate(token_positions):\n",
    "            if not (token_end <= start or token_start >= end):\n",
    "                entity_tokens.append(i)\n",
    "        \n",
    "        # Assign BIO tags\n",
    "        if entity_tokens:\n",
    "            for j, token_idx in enumerate(entity_tokens):\n",
    "                if j == 0:\n",
    "                    ner_tags[token_idx] = 'B-MOUNTAIN'\n",
    "                else:\n",
    "                    ner_tags[token_idx] = 'I-MOUNTAIN'\n",
    "    \n",
    "    return tokens, ner_tags\n",
    "\n",
    "# Test\n",
    "tokens, tags = create_ner_tags(text, markers)\n",
    "\n",
    "print(\"Token-level NER tags:\")\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"  {token:20s} {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1584 samples\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    markers = parse_marker(row['marker'])\n",
    "    tokens, ner_tags = create_ner_tags(text, markers)\n",
    "    \n",
    "    if tokens:\n",
    "        processed_data.append({\n",
    "            'tokens': tokens,\n",
    "            'ner_tags': ner_tags\n",
    "        })\n",
    "\n",
    "print(f\"Processed {len(processed_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag distribution:\n",
      "  O: 21932 (98.2%)\n",
      "  B-MOUNTAIN: 235 (1.1%)\n",
      "  I-MOUNTAIN: 168 (0.8%)\n",
      "\n",
      "Total entities: 235\n",
      "Samples with entities: 226/1584\n",
      "Average tokens per sample: 14.1\n"
     ]
    }
   ],
   "source": [
    "# Count tags\n",
    "all_tags = [tag for item in processed_data for tag in item['ner_tags']]\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "print(\"Tag distribution:\")\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"  {tag}: {count} ({count/len(all_tags)*100:.1f}%)\")\n",
    "\n",
    "# Count entities\n",
    "num_entities = sum(1 for item in processed_data for tag in item['ner_tags'] if tag == 'B-MOUNTAIN')\n",
    "samples_with_entities = sum(1 for item in processed_data if 'B-MOUNTAIN' in item['ner_tags'])\n",
    "\n",
    "print(f\"\\nTotal entities: {num_entities}\")\n",
    "print(f\"Samples with entities: {samples_with_entities}/{len(processed_data)}\")\n",
    "print(f\"Average tokens per sample: {len(all_tags)/len(processed_data):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['The', 'Carpathian', 'Mountains', 'are', 'a', 'vital', 'part', 'of', \"Europe's\", 'natural', 'heritage.']\n",
      "Original labels: ['O', 'B-MOUNTAIN', 'I-MOUNTAIN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "BERT tokens: ['[CLS]', 'The', 'Car', '##pathian', 'Mountains', 'are', 'a', 'vital', 'part', 'of', 'Europe', \"'\", 's', 'natural', 'heritage', '.', '[SEP]']\n",
      "Word IDs: [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 9, 10, 10, None]\n",
      "\n",
      "Aligned labels: [-100, 0, 1, -100, 2, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, -100, -100]\n",
      "(-100 = ignored in loss calculation)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Example with subword tokens\n",
    "example = processed_data[22]\n",
    "tokens = example['tokens']\n",
    "labels = example['ner_tags']\n",
    "\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"Original labels: {labels}\")\n",
    "\n",
    "# Tokenize\n",
    "encoding = tokenizer(tokens, is_split_into_words=True, return_tensors='pt')\n",
    "word_ids = encoding.word_ids()\n",
    "\n",
    "print(f\"\\nBERT tokens: {tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])}\")\n",
    "print(f\"Word IDs: {word_ids}\")\n",
    "\n",
    "# Align labels\n",
    "label2id = {'O': 0, 'B-MOUNTAIN': 1, 'I-MOUNTAIN': 2}\n",
    "aligned_labels = []\n",
    "previous_word_idx = None\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    if word_idx is None:\n",
    "        aligned_labels.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        aligned_labels.append(label2id[labels[word_idx]])\n",
    "    else:\n",
    "        aligned_labels.append(-100)\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "print(f\"\\nAligned labels: {aligned_labels}\")\n",
    "print(\"(-100 = ignored in loss calculation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1267 samples\n",
      "Val: 317 samples\n",
      "\n",
      "Train entities: 189\n",
      "Val entities: 46\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(processed_data)\n",
    "split_idx = int(len(processed_data) * 0.8)\n",
    "\n",
    "train_data = processed_data[:split_idx]\n",
    "val_data = processed_data[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(train_data)} samples\")\n",
    "print(f\"Val: {len(val_data)} samples\")\n",
    "\n",
    "# Statistics per split\n",
    "train_entities = sum(1 for item in train_data for tag in item['ner_tags'] if tag == 'B-MOUNTAIN')\n",
    "val_entities = sum(1 for item in val_data for tag in item['ner_tags'] if tag == 'B-MOUNTAIN')\n",
    "\n",
    "print(f\"\\nTrain entities: {train_entities}\")\n",
    "print(f\"Val entities: {val_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample examples with mountains:\n",
      "\n",
      "Example 1:\n",
      "  Text: The Himalayas are a sacred place for many people, and there are many monasteries and temples to be found in the region. #tibetanadventure #mountainspirit\n",
      "  Tags: ['O', 'B-MOUNTAIN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Example 2:\n",
      "  Text: Gaze upon the snow-capped peaks of Mount Rainier, a dormant volcano in Washington state, and feel the power of nature's slumbering giant. #mountainviews #washingtonwonders\n",
      "  Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNTAIN', 'I-MOUNTAIN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Example 3:\n",
      "  Text: Embarking on an unforgettable journey through the rugged terrain of the Appalachian Mountains #hikingtrail #mountainexplorer\n",
      "  Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNTAIN', 'I-MOUNTAIN', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample examples with mountains:\\n\")\n",
    "\n",
    "count = 0\n",
    "for item in train_data:\n",
    "    if 'B-MOUNTAIN' in item['ner_tags']:\n",
    "        print(f\"Example {count+1}:\")\n",
    "        print(f\"  Text: {' '.join(item['tokens'])}\")\n",
    "        print(f\"  Tags: {item['ner_tags']}\")\n",
    "        print()\n",
    "        count += 1\n",
    "        if count >= 3:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
